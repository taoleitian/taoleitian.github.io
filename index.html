<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hongwei Xue</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hongwei Xue</name>
              </p>
              <p>I am a senior undergraduate at <a href="https://www.whu.edu.cn/">Wuhan University</a> (WHU), My advisors are <a href="http://iip.whu.edu.cn/">Zhenzhong Chen</a>. I am pleasure to have experienced the research internship at  <a href="https://www.wisc.edu/">University of Wisconsin‚ÄìMadison</a>, under the supervision of <a href="https://pages.cs.wisc.edu/~sharonli/">Sharon Li</a>.
              </p>
              <p>
                Currently I'm also working as a research intern at <a href="https://www.shlab.org.cn/">Shanghai AI lab</a>. My mentor at MSRA is <a href="https://gaopengpjlab.github.io/">Peng Gao</a>.
              </p>
              <p style="text-align:center">
                <a href="tianleitao@whu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=F5S6V6sAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/taoleitian">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/hongwei.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/hongwei-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, and multimedia. Much of my research is about Vision-and-Language Pre-training. Representative papers are <span class="highlight">highlighted</span>. (* indicates equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
          <tr bgcolor="#ffffd0"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/hdvila.png"><img src="images/hdvila.png" alt="hdvila" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2111.10337">
                <papertitle>Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions</papertitle>
              </a>
              <br>
              <strong>Hongwei Xue*</strong>, 
              <a href="https://tiankaihang.github.io/">Tiankai Hang*</a>,
              <a href="https://1900zyh.github.io/">Yanhong Zeng*</a>,
              Yuchong Sun*, 
              <a href="https://www.microsoft.com/en-us/research/people/libei/">Bei Liu</a>, 
              <a href="https://www.microsoft.com/en-us/research/people/huayan/">Huan Yang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
              <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2111.10337.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2111.10337">[arXiv]</a>
	      <a href="https://github.com/microsoft/XPretrain">[Code]</a>
              <p>We collect a large dataset which is the first high-resolution dataset including 371.5k hours of 720p videos and the most diversified dataset covering 15 popular YouTube categories. </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/visualparsing.png"><img src="images/visualparsing.png" alt="visualparsing" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/23fa71cc32babb7b91130824466d25a5-Abstract.html">
                <papertitle>Probing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training</papertitle>
              </a>
              <br>
              <strong>Hongwei Xue</strong>, 
              <a href="https://hypjudy.github.io/">Yupan Huang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/libei/">Bei Liu</a>, 
              <a href="https://houwenpeng.com/">Houwen Peng</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
              <a href="http://staff.ustc.edu.cn/~lihq/en/">Houqiang Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>NeurIPS</em>, 2021
              <br>
              <a href="https://proceedings.neurips.cc/paper/2021/file/23fa71cc32babb7b91130824466d25a5-Paper.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2106.13488">[arXiv]</a>
              <a href="https://proceedings.neurips.cc/paper/2021/file/23fa71cc32babb7b91130824466d25a5-Supplemental.pdf">[Supp]</a>
              <a href="https://nips.cc/virtual/2021/poster/27599">[Presentation]</a>
              <p>We propose a fully Transformer model for Vision-and-Language pre-training and explore to study the inter-modal interaction.</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/landscape.jpg"><img src="images/landscape.jpg" alt="landscape" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475421">
                <papertitle>Learning fine-grained motion embedding for landscape animation</papertitle>
              </a>
              <br>
              <strong>Hongwei Xue</strong>, 
              <a href="https://hypjudy.github.io/">Yupan Huang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/libei/">Bei Liu</a>, 
              <a href="https://www.microsoft.com/en-us/research/people/huayan/">Huan Yang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
              <a href="http://staff.ustc.edu.cn/~lihq/en/">Houqiang Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>ACM MM Oral</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2109.02216.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2109.02216">[arXiv]</a>
              <p>We propose a model named FGLA to generate high-quality and realistic videos by learning Fine-Grained motion embedding for Landscape Animation.</p>
            </td>
          </tr>


          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/unifiying.png"><img src="images/unifiying.png" alt="unifiying" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3481540">
                <papertitle>Unifying Multimodal Transformer for Bi-directional Image and Text Generation</papertitle>
              </a>
              <br> 
              <a href="https://hypjudy.github.io/">Yupan Huang</a>,
              <strong>Hongwei Xue</strong>,
              <a href="https://www.microsoft.com/en-us/research/people/libei/">Bei Liu</a>, 
              Yutong Lu
              <br>
              <em>ACM MM</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2110.09753.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2110.09753">[arXiv]</a>
              <a href="https://github.com/researchmm/generate-it">[Code]</a>
              
              <p>In this work, we propose a unified image-and-text generative framework based on a single multimodal model to jointly study the bi-directional tasks.</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/challenge.png"><img src="images/challenge.png" alt="Semantic Tag Augmented XlanV Model for Video Captioning" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3479228">
                <papertitle>Semantic Tag Augmented XlanV Model for Video Captioning</papertitle>
              </a>
              <br> 
              <a href="https://scholar.google.com.hk/citations?user=MGMKjMoAAAAJ">Yiqing Huang*</a>,
              <strong>Hongwei Xue*</strong>,
              <a href="https://jschenthu.weebly.com/">Jiansheng Chen</a>,
              <a href="https://scholar.google.com/citations?user=32hwVLEAAAAJ">Huimin Ma</a>,
              Hongbing Ma
              <br>
              <em>ACM MM</em>, 2021
              <br>
              
              <p>We propose to leverage the semantic tags to bridge the gap between the modalities of vision and language rather than directly concatenating or attending to the visual and linguistic features.</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/sednet.png"><img src="images/sednet.png" alt="Sed-Net" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9102910">
                <papertitle>Sed-Net: Detecting Multi-Type Edits Of Images</papertitle>
              </a>
              <br> 
              <strong>Hongwei Xue</strong>,
              Haomiao Liu,
              Jun Li,
              <a href="http://staff.ustc.edu.cn/~lihq/en/">Houqiang Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>ICME</em>, 2020
              <br>
              
              <p>We propose a deep Siamese network model to to classify different types of image edits between an original image and an edited image.</p>
            </td>
          </tr>

          

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td width="0%" valign="center">
              <ul>
                <li>National Scholarship in 2021.</li>
                <li>Scholarship for Excellent Students in 2016, 2017, 2018. Freshmen Scholarship 2015.</li>
              </ul>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Steal from <a href="https://jonbarron.info/">Jon Barron's website</a>.
                
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
